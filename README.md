# HH Parser – интерактивная аналитика вакансий (MVP)

Этот проект парсит вакансии с HeadHunter (HH.ru) и строит по ним интерактивную аналитику: направления в IT, стек технологий, требования по опыту, зарплатные вилки и т.п.  
Основной пользовательский интерфейс сейчас реализован как **Streamlit‑приложение** в папке `filter city/Chart/`. Проект находится на стадии **сырого MVP**, логика парсинга и графики активно дорабатываются.

## Статус проекта

**Важно: проект в активной разработке.**

- Точность парсера, классификации и аналитики **пока не гарантируется**.
- Структура директорий и API функций могут меняться.
- Некоторые части кода остаются экспериментальными (в т.ч. папка `experiments_page/` и папка `use model/`).

## Основной функционал

- **Парсинг вакансий HH.ru**
  - Поиск по городу или по всей России.
  - Сбор описаний вакансий через HH API + HTML-страницы.
  - Сохранение выборки в Excel.
- **Классификация и аналитика**
  - Лемматизация текстов описаний с помощью `natasha`.
  - Выделение навыков (технологий) и категорий ролей (backend, data, analytics и т.д.).
  - Распределение вакансий по направлениям, технологиям, опыту и зарплатным диапазонам.
- **Интерактивный дашборд (Streamlit)**
  - Главная страница: запуск парсинга, таблица вакансий, быстрые фильтры по направлениям, экспорт в Excel.
  - Загрузка внешних файлов (CSV/Excel/JSON) с дальнейшим пересчётом навыков и категорий.
  - Отдельные страницы с аналитикой по зарплатам, опыту, навыкам и направлениям.

## Структура проекта (основное)

### `filter city/Chart/` – Streamlit‑приложение

- `main_nav.py` – точка входа Streamlit, навигация по страницам (`st.navigation`):
  - `main_page/main.py` – **главная страница**:
    - запуск парсинга HH (по городу или по всей РФ);
    - NLP‑обработка через `natasha` (лемматизация);
    - извлечение навыков из описаний вакансий;
    - классификация вакансий по направлениям;
    - базовая таблица вакансий + экспорт в Excel;
    - загрузка внешних файлов и их анализ.
  - `salary_page/salary.py` – аналитика зарплат:
    - распределение «зарплата от» по категориям (boxplot);
    - медианные вилки по направлениям;
    - зависимость зарплаты от требуемого опыта.
  - `experience_page/experience.py` – распределение требований по опыту:
    - доли вакансий по уровням HH («Нет опыта», «1–3 года», …) в разрезе направлений;
    - подсветка направлений с низким/высоким порогом входа.
  - `skills_page/skills.py` – аналитика стека технологий:
    - TOP‑технологий в выборке;
    - распределение навыков по направлениям;
    - связь «навык ↔ средняя зарплата (от)».
  - `areas_page/area_page.py` – сводная аналитика по направлениям (общее число вакансий, лидеры рынка).
  - `experiments_page/experiments.py` – экспериментальные графики и идеи (может меняться или ломаться).

Все страницы работают поверх общего датафрейма `st.session_state['vacancies_df']`, который формируется на главной странице.

### `filter city/` – низкоуровневый парсер и старые интерфейсы

- `save_csv.py` / `save_csv_2.py` – базовые функции парсинга HH API:
  - поиск по региону/городу;
  - сбор вакансий и описаний;
  - сохранение в Excel.
- `parser_ui.py` – **настольный интерфейс на Tkinter**:
  - ручной ввод города и ключевого слова;
  - таблица вакансий и просмотр подробностей;
  - экспорт в Excel.
- `app.py` – **веб-интерфейс на Flask**:
  - форма поиска вакансий;
  - базовая аналитика и экспорт в Excel.

Streamlit‑приложение логически опирается на те же идеи и структуры данных, но реализовано отдельно и является **основным способом работы** с проектом.

### Другие директории

- `all city/`
  - `parse_country.py` – пример парсинга вакансий по ключевому слову по всей России (или выбранному региону) с сохранением в CSV.
- `use model/`
  - эксперименты с моделью `facebook/bart-large-mnli` (zero‑shot классификация вакансий по ролям, отдельные скрипты для проверки GPU и разметки CSV).

## Зависимости

Базовые зависимости (см. `requirements.txt`):

```text
requests
torch
transformers
pandas
openpyxl
beautifulsoup4
flask
```

Для запуска Streamlit‑дашборда также требуются (могут быть добавлены в `requirements.txt` позже):

```text
streamlit
plotly
numpy
natasha
xlsxwriter
```

Установка (рекомендуется виртуальное окружение):

```bash
pip install -r requirements.txt
pip install streamlit plotly numpy natasha xlsxwriter
```

## Как запустить Streamlit‑приложение

1. Клонировать репозиторий и установить зависимости.
2. Перейти в папку с дашбордом:
   - Windows PowerShell:
     ```powershell
     cd "D:\Проекты\HH Парсер\filter city\Chart"
     ```
3. Запустить Streamlit:
   ```bash
   streamlit run main_nav.py
   ```
4. Открыть браузер по адресу, который покажет Streamlit (обычно `http://localhost:8501`).

### Типичный сценарий работы

1. На **главной странице** задать:
   - город (или включить поиск по всей России);
   - ключевое слово/стек (например, `Python`, `Data Analyst` и т.п.);
   - глубину поиска (количество страниц).
2. Нажать «Начать сбор данных» и дождаться завершения.
3. Посмотреть таблицу вакансий, переключаться по категориям, скачать Excel.
4. Перейти на вкладки:
   - «Зарплата аналитика» – анализ зарплат;
   - «Опыт аналитика» – требования по стажу;
   - «Навыки аналитика» – стек технологий;
   - «Направления аналитика» и «Эксперименты» – дополнительные графики.
5. При необходимости загрузить внешний файл (Excel/CSV/JSON) на главной странице и прогнать по той же аналитике.

## Ограничения и планы

- Парсер зависит от структуры HH API и HTML‑страниц – при изменениях на HH может потребоваться правка кода.
- Классификация направлений и навыков реализована через словари и эвристики, а не через обученную модель; результаты нужно трактовать как **ориентировочные**.
- Скорость работы при глубоком поиске и большом количестве вакансий может быть низкой (дополнительные запросы за полными описаниями и NLP‑обработка).

Планируется:

- улучшать словари и правила классификации;
- донастроить и документировать все графики;
- привести зависимости в порядок и добавить автотесты для ключевых функций парсинга.
