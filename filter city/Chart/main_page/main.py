import streamlit as st
import pandas as pd
import requests
import time
from bs4 import BeautifulSoup
from io import BytesIO
from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, Doc
import re
from main_page.data import SKILL_MAP, CATEGORIES, PRIORITY
from main_page.setting.city_to_id import CITY_TO_ID

# =========================================================
# 1. –ù–ê–°–¢–†–û–ô–ö–ê NLP (NATASHA) –ò –°–õ–û–í–ê–†–¨ –¢–ï–•–ù–û–õ–û–ì–ò–ô
# =========================================================
segmenter = Segmenter()
morph_vocab = MorphVocab()
emb = NewsEmbedding()
tagger = NewsMorphTagger(emb)

# SKILL_MAP = {}

def clean_and_lemmatize(text):
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π, NaN –∏–ª–∏ –Ω–µ —Å—Ç—Ä–æ–∫–∞ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
    if pd.isna(text) or not isinstance(text, str) or not text.strip():
        return ""
    
    # –¢–µ–ø–µ—Ä—å –±–µ–∑–æ–ø–∞—Å–Ω–æ –≤—ã–∑—ã–≤–∞–µ–º .lower() –∏ –∑–∞–º–µ–Ω—ã
    clean_text = text.lower().replace('-', ' ').replace('/', ' ')
    
    doc = Doc(clean_text)
    doc.segment(segmenter)
    doc.tag_morph(tagger)
    for token in doc.tokens:
        token.lemmatize(morph_vocab)
    return " ".join([_.lemma for _ in doc.tokens])

def extract_skills(lemmatized_text):
    if not lemmatized_text: return []
    
    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    text = lemmatized_text.lower()
    for char in "()/,[]": text = text.replace(char, " ")
    
    # –°—é–¥–∞ –±—É–¥–µ–º —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–∞–π–¥–µ–Ω–Ω–æ–µ: { "Languages": ["python"], "Databases": ["sql"] }
    found_by_category = {cat: [] for cat in SKILL_MAP.keys()}
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–∞–≤—ã–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    all_skills_to_search = []
    for category, skills in SKILL_MAP.items():
        for s in skills:
            all_skills_to_search.append({"name": s, "cat": category})
            
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ, —á—Ç–æ–±—ã —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–π—Ç–∏ C++, –∞ –Ω–µ C
    all_skills_to_search.sort(key=lambda x: len(x["name"]), reverse=True)

    for skill_item in all_skills_to_search:
        skill_clean = skill_item["name"].lower()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω (–≥–∏–±–∫–∏–π –¥–ª—è C++ –∏ –æ–±—ã—á–Ω—ã–π –¥–ª—è SQL)
        if "++" in skill_clean or "#" in skill_clean:
            pattern = r"".join([re.escape(char) + r"\s*" for char in skill_clean]).strip()
        else:
            pattern = rf"\b{re.escape(skill_clean)}\b"
        
        if re.search(pattern, text):
            found_by_category[skill_item["cat"]].append(skill_item["name"])
            text = re.sub(pattern, " ", text) # –£–¥–∞–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞

    # –§–û–†–ú–ò–†–£–ï–ú –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ô –°–ü–ò–°–û–ö
    final_list = []
    # –ò–¥–µ–º —Å—Ç—Ä–æ–≥–æ –ø–æ –Ω–∞—à–µ–º—É –ø–æ—Ä—è–¥–∫—É SKILL_ORDER
    SKILL_ORDER = ["Languages", "Frameworks", "Databases", "Infrastructure", "Tools", "Methodologies", "Security"]
    
    for category in SKILL_ORDER:
        if category in found_by_category:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤–Ω—É—Ç—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
            final_list.extend(sorted(found_by_category[category]))
            
    return final_list # –¢–µ–ø–µ—Ä—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —è–∑—ã–∫–æ–≤

# =========================================================
# 2. –£–°–û–í–ï–†–®–ï–ù–°–¢–í–û–í–ê–ù–ù–´–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†
# =========================================================
# CATEGORIES = {}
# PRIORITY = []

def classify_vacancy(title, description_lemmatized):
    # –ó–∞—â–∏—Ç–∞ –æ—Ç NaN –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
    if pd.isna(title) or not isinstance(title, str):
        title = "–±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
        
    clean_title = clean_and_lemmatize(title)
    scores = {cat: 0 for cat in CATEGORIES}
    
    for category in PRIORITY:
        for keyword in CATEGORIES[category]:
            if keyword in clean_title: 
                scores[category] += 10
                
    if max(scores.values()) == 0:
        # –ó–∞—â–∏—Ç–∞ –æ—Ç NaN –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
        desc = description_lemmatized if isinstance(description_lemmatized, str) else ""
        for category in PRIORITY:
            for keyword in CATEGORIES[category]:
                if keyword in desc: 
                    scores[category] += 1
                    
    best_cat = max(scores, key=scores.get)
    return best_cat if scores[best_cat] > 0 else "Other"

# =========================================================
# 3. –§–£–ù–ö–¶–ò–ò –ü–ê–†–°–ò–ù–ì–ê
# =========================================================
@st.cache_data
def get_area_id_by_city(city_name: str) -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç area_id –≥–æ—Ä–æ–¥–∞ –ø–æ –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—é.
    –ï—Å–ª–∏ –≥–æ—Ä–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç–æ–µ –∏–º—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç '1' (–ú–æ—Å–∫–≤–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é).
    """
    if not city_name:
        return "1"
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, —á—Ç–æ–±—ã –∏—Å–∫–∞—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ
    city_key = city_name.strip().lower()
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º ID –≥–æ—Ä–æ–¥–∞ –∏–∑ —Å–ª–æ–≤–∞—Ä—è –∏–ª–∏ '1' –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    return CITY_TO_ID.get(city_key, "1")

"wwww"

def fetch_full_description(url):
    try:
        r = requests.get(url, headers={"User-Agent": "HH-Parser/1.0"}, timeout=5)
        soup = BeautifulSoup(r.text, "html.parser")
        block = soup.find("div", {"data-qa": "vacancy-description"}) or soup.find("div", class_="g-user-content")
        return block.get_text(separator="\n").strip() if block else ""
    except: return ""

def start_parsing(text, city_name, max_pages, all_russia):
    area_id = "1" if all_russia else get_area_id_by_city(city_name)
    all_vacancies = []
    status_container = st.empty()
    progress_bar = st.progress(0)

    for page in range(max_pages):
        params = {"text": text, "area": area_id, "per_page": 20, "page": page}
        try:
            res = requests.get("https://api.hh.ru/vacancies", params=params)
            if res.status_code != 200: break
            items = res.json().get("items", [])
            if not items: break

            for item in items:
                name = item.get("name")
                url = item.get("alternate_url")
                status_container.info(f"üõ∞Ô∏è –†–µ–≥–∏–æ–Ω: {'–†–æ—Å—Å–∏—è' if all_russia else city_name} | –ê–Ω–∞–ª–∏–∑: {name[:30]}...")
                
                desc = fetch_full_description(url)
                desc_lemmatized = clean_and_lemmatize(desc)
                salary = item.get("salary")
                found_skills = extract_skills(desc_lemmatized)
                
                all_vacancies.append({
                    "name": name,
                    "category": classify_vacancy(name, desc_lemmatized),
                    "company": item.get("employer", {}).get("name"),
                    "salary_from": salary["from"] if salary else None,
                    "salary_to": salary["to"] if salary else None,
                    "currency": salary["currency"] if salary else None,
                    "experience": item.get("experience", {}).get("name", "–ù–µ —É–∫–∞–∑–∞–Ω"),
                    "skills": ", ".join(found_skills),
                    "url": url,
                    "description": desc,
                    "lemmatized_content": desc_lemmatized  # –ù–û–í–ê–Ø –ö–û–õ–û–ù–ö–ê
                })
                time.sleep(0.05)
        except: break
        progress_bar.progress((page + 1) / max_pages)
    
    status_container.success(f"‚úÖ –°–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω! –ù–∞–π–¥–µ–Ω–æ {len(all_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π.")
    return pd.DataFrame(all_vacancies), None

# =========================================================
# 4. –ò–ù–¢–ï–†–§–ï–ô–° STREAMLIT
# =========================================================
st.set_page_config(page_title="Skill Hunter 2026", layout="wide")

# –°–∞–π–¥–±–∞—Ä –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫
with st.sidebar:
    st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
    all_russia = st.checkbox("üåç –ò—Å–∫–∞—Ç—å –ø–æ –≤—Å–µ–π –†–æ—Å—Å–∏–∏", value=False)
    
    if not all_russia:
        city_in = st.text_input("–ì–æ—Ä–æ–¥", value="–ú–æ—Å–∫–≤–∞")
    else:
        city_in = ""
        st.info("–ü–æ–∏—Å–∫ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω –ø–æ –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–µ")
        
    query_in = st.text_input("–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ (–°—Ç–µ–∫/–†–æ–ª—å)", value="Python")
    limit_in = st.slider("–ì–ª—É–±–∏–Ω–∞ –ø–æ–∏—Å–∫–∞ (—Å—Ç—Ä–∞–Ω–∏—Ü)", 1, 50, 5)
    
    st.divider()
    btn_start = st.button("üöÄ –ù–∞—á–∞—Ç—å —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö", use_container_width=True)

# –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
st.header("üîé –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ IT-—Ä—ã–Ω–∫–∞")

if 'vacancies_df' not in st.session_state:
    st.session_state['vacancies_df'] = None
if 'selected_cat' not in st.session_state:
    st.session_state['selected_cat'] = "–í—Å–µ"

if btn_start:
    df_result, err = start_parsing(query_in, city_in, limit_in, all_russia)
    if err: st.error(err)
    else:
        st.session_state['vacancies_df'] = df_result

if st.session_state['vacancies_df'] is not None:
    df = st.session_state['vacancies_df']
    
    # –°–µ–∫—Ü–∏—è –±—ã—Å—Ç—Ä—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤-–∫–Ω–æ–ø–æ–∫
    cat_counts = df["category"].value_counts()
    st.write("### –ë—ã—Å—Ç—Ä—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º:")
    
    cols = st.columns(6)
    if cols[0].button("üåê –í—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏"): 
        st.session_state['selected_cat'] = "–í—Å–µ"
    
    for i, (cat, count) in enumerate(cat_counts.items()):
        if cols[(i+1) % 6].button(f"{cat}: {count}"):
            st.session_state['selected_cat'] = cat

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –≤—ã–≤–æ–¥ —Ç–∞–±–ª–∏—Ü—ã
    display_df = df if st.session_state['selected_cat'] == "–í—Å–µ" else df[df["category"] == st.session_state['selected_cat']]
    
    st.info(f"–û—Ç–æ–±—Ä–∞–∂–µ–Ω–æ: **{st.session_state['selected_cat']}** | –í–∞–∫–∞–Ω—Å–∏–π: **{len(display_df)}**")
    st.dataframe(
        display_df.drop(columns=['description']), 
        use_container_width=True,
        column_config={
            "url": st.column_config.LinkColumn("–°—Å—ã–ª–∫–∞"),
            "skills": st.column_config.TextColumn("–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", width="large")
        }
    )

    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False)
    st.download_button("üì• –°–∫–∞—á–∞—Ç—å –±–∞–∑—É –≤ Excel", output.getvalue(), f"hh_export_{query_in}.xlsx")
    
    
# =========================================================
# 5. –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –û–ë–†–ê–ë–û–¢–ß–ò–ö –§–ê–ô–õ–û–í (–£–õ–¨–¢–†–ê-–ó–ê–©–ò–¢–ê)
# =========================================================
st.divider()
st.subheader("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –≤–Ω–µ—à–Ω–µ–π –±–∞–∑—ã")

up_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ Excel/CSV/JSON", type=['xlsx', 'csv', 'json'])

if up_file:
    try:
        # 1. –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        if up_file.name.endswith('.xlsx'):
            df_file = pd.read_excel(up_file)
        elif up_file.name.endswith('.json'):
            df_file = pd.read_json(up_file)
        else:
            df_file = pd.read_csv(up_file)

        # 2. –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–ß–ò–°–¢–ö–ê –¢–ò–ü–û–í –î–ê–ù–ù–´–•
        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        name_map = {'title': 'name', '–≤–∞–∫–∞–Ω—Å–∏—è': 'name', '–¥–æ–ª–∂–Ω–æ—Å—Ç—å': 'name'}
        df_file = df_file.rename(columns=name_map)
        
        # –ì–ê–†–ê–ù–¢–ò–†–£–ï–ú, –ß–¢–û –¢–ï–ö–°–¢–û–í–´–ï –ö–û–õ–û–ù–ö–ò ‚Äî –≠–¢–û –°–¢–†–û–ö–ò
        # –î–∞–∂–µ –µ—Å–ª–∏ —Ç–∞–º NaN, –æ–Ω–∏ —Å—Ç–∞–Ω—É—Ç —Å—Ç—Ä–æ–∫–æ–π "nan"
        if 'name' in df_file.columns:
            df_file['name'] = df_file['name'].astype(str).replace('nan', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
        else:
            df_file['name'] = "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

        if 'description' in df_file.columns:
            df_file['description'] = df_file['description'].astype(str).replace('nan', '')
        else:
            df_file['description'] = ""

        # –ú–∞–ø–ø–∏–Ω–≥ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        df_file = df_file.rename(columns={'–æ–ø—ã—Ç': 'experience', '–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã': 'experience', 'exp': 'experience'})
        if 'experience' not in df_file.columns:
            df_file['experience'] = "–ù–µ —É–∫–∞–∑–∞–Ω"
        else:
            df_file['experience'] = df_file['experience'].astype(str).replace('nan', '–ù–µ —É–∫–∞–∑–∞–Ω')

        st.success(f"üìä –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω. –°—Ç—Ä–æ–∫: {len(df_file)}")

        # 3. –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
        if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π –ø–µ—Ä–µ—Å—á–µ—Ç", key="analyze_btn"):
            with st.status("üîÑ –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ 1000+ –≤–∞–∫–∞–Ω—Å–∏–π...") as status:
                
                # –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–Ø –° –ò–ù–î–ò–ö–ê–¢–û–†–û–ú (–≤–∞–∂–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤)
                st.write("üß† –†–∞–±–æ—Ç–∞–µ—Ç Natasha (–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)...")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏, —Ç–∞–∫ –∫–∞–∫ 1000 —Å—Ç—Ä–æ–∫ ‚Äî —ç—Ç–æ –¥–æ–ª–≥–æ
                df_file['lemmatized_content'] = [clean_and_lemmatize(text) for text in df_file['description']]
                
                st.write("üîç –ü–æ–∏—Å–∫ –Ω–∞–≤—ã–∫–æ–≤...")
                df_file['skills_list'] = df_file['lemmatized_content'].apply(extract_skills)
                df_file['skills'] = df_file['skills_list'].apply(lambda x: ", ".join(x))
                
                st.write("üóÇÔ∏è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–æ–ª–µ–π...")
                df_file['category'] = df_file.apply(
                    lambda row: classify_vacancy(row['name'], row['lemmatized_content']), axis=1
                )
                
                status.update(label="‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!", state="complete")

            st.session_state['vacancies_df'] = df_file
            st.rerun()

    except Exception as e:
        # –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç —Ç–µ–±–µ —É–≤–∏–¥–µ—Ç—å, –Ω–∞ –∫–∞–∫–æ–π –∏–º–µ–Ω–Ω–æ —Å—Ç—Ä–æ–∫–µ –∏–ª–∏ –∫–æ–ª–æ–Ω–∫–µ –æ—à–∏–±–∫–∞
        st.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ñ–∞–π–ª–∞: {e}")