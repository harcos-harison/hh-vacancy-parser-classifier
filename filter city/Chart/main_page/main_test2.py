import streamlit as st
import pandas as pd
import requests
import time
from bs4 import BeautifulSoup
from io import BytesIO
from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, Doc

# =========================================================
# 1. –ù–ê–°–¢–†–û–ô–ö–ê NLP (NATASHA)
# =========================================================
segmenter = Segmenter()
morph_vocab = MorphVocab()
emb = NewsEmbedding()
tagger = NewsMorphTagger(emb)

def clean_and_lemmatize(text):
    """–ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞."""
    if not text:
        return ""
    doc = Doc(str(text).lower().replace('-', ' ').replace('/', ' '))
    doc.segment(segmenter)
    doc.tag_morph(tagger)
    for token in doc.tokens:
        token.lemmatize(morph_vocab)
    return " ".join([_.lemma for _ in doc.tokens])

# =========================================================
# 2. –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–õ–û–í–ê–†–¨ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò
# =========================================================
CATEGORIES = {
    "AI, ML & LLM": ["ai", "ml", "llm", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "vision", "cv", "prompt", "–ø—Ä–æ–º–ø—Ç", "rpa"],
    "QA & Automation": ["qa", "—Ç–µ—Å—Ç", "–∞–≤—Ç–æ—Ç–µ—Å—Ç", "—Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫", "automation", "–∏—Å–ø—ã—Ç–∞–Ω–∏–µ", "manual", "selenium", "–Ω–∞–≥—Ä—É–∑–æ—á–Ω—ã–π"],
    "Cybersecurity": ["–ø–µ–Ω—Ç–µ—Å—Ç–µ—Ä", "pentest", "security", "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "reverse", "—Ä–µ–≤–µ—Ä—Å", "appsec", "soc", "siem", "–∏–±"],
    "Electronics & Hardware": ["—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫", "—ç–ª–µ–∫—Ç—Ä–æ–Ω—â–∏–∫", "—Ä–∞–¥–∏–æ—ç–ª–µ–∫—Ç—Ä–æ–Ω", "–ø–ª–∏—Å", "fpga", "sdr", "dsp", "—Å—Ö–µ–º–æ—Ç–µ—Ö–Ω–∏–∫", "hardware", "embedded"],
    "Network & SysAdmin": ["—Å–µ—Ç–µ–≤–æ–π", "network", "—Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä", "sysadmin", "linux", "voip", "asterisk", "kubernetes", "k8s"],
    "Analytics & Data Science": ["–∞–Ω–∞–ª–∏—Ç–∏–∫", "analytics", "bi", "sql", "data scientist", "—Å—É–±–¥", "–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö", "–º–∞—Ç–µ–º–∞—Ç–∏–∫"],
    "Software Development": ["—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫", "developer", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç", "backend", "frontend", "fullstack", "gamedev", "python", "java", "c#"],
    "Education & HR": ["–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å", "—É—á–∏—Ç–µ–ª—å", "–º–µ—Ç–æ–¥–∏—Å—Ç", "–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫", "—Ä–µ–∫—Ä—É—Ç–µ—Ä", "recruiter", "hr", "–æ–±—É—á–µ–Ω–∏–µ"],
    "Support & Management": ["–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "helpdesk", "l2", "—Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ", "–º–µ–Ω–µ–¥–∂–µ—Ä", "product", "project", "cto", "lead"]
}

PRIORITY = ["AI, ML & LLM", "QA & Automation", "Cybersecurity", "Electronics & Hardware", 
            "Network & SysAdmin", "Analytics & Data Science", "Software Development", 
            "Education & HR", "Support & Management"]

def classify_vacancy(title, description):
    """–ì–∏–±—Ä–∏–¥–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –∑–∞–≥–æ–ª–æ–≤–æ–∫ + –æ–ø–∏—Å–∞–Ω–∏–µ."""
    clean_title = clean_and_lemmatize(title)
    scores = {cat: 0 for cat in CATEGORIES}

    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
    for category in PRIORITY:
        for keyword in CATEGORIES[category]:
            if keyword in clean_title:
                scores[category] += 10

    # 2. –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
    if max(scores.values()) == 0:
        clean_desc = clean_and_lemmatize(description)
        for category in PRIORITY:
            for keyword in CATEGORIES[category]:
                if keyword in clean_desc:
                    scores[category] += 1

    best_cat = max(scores, key=scores.get)
    return best_cat if scores[best_cat] > 0 else "Other"

# =========================================================
# 3. –§–£–ù–ö–¶–ò–ò –ü–ê–†–°–ò–ù–ì–ê
# =========================================================
@st.cache_data
def get_area_id_by_city(city_name):
    try:
        res = requests.get("https://api.hh.ru/areas")
        def search(areas_list):
            for a in areas_list:
                if a["name"].lower() == city_name.lower(): return a["id"]
                if a.get("areas"):
                    r = search(a["areas"])
                    if r: return r
            return None
        return search(res.json())
    except: return None

def fetch_full_description(url):
    try:
        r = requests.get(url, headers={"User-Agent": "HH-Parser/1.0"}, timeout=5)
        soup = BeautifulSoup(r.text, "html.parser")
        block = soup.find("div", {"data-qa": "vacancy-description"}) or soup.find("div", class_="g-user-content")
        return block.get_text(separator="\n").strip() if block else ""
    except: return ""

def start_parsing(text, city_name, max_pages):
    area_id = get_area_id_by_city(city_name)
    if not area_id: return None, "–ì–æ—Ä–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω"

    all_vacancies = []
    status_container = st.empty()
    progress_bar = st.progress(0)

    for page in range(max_pages):
        params = {"text": text, "area": area_id, "per_page": 20, "page": page}
        res = requests.get("https://api.hh.ru/vacancies", params=params)
        if res.status_code != 200: break
        
        items = res.json().get("items", [])
        if not items: break

        for item in items:
            name = item.get("name")
            url = item.get("alternate_url")
            status_container.info(f"‚è≥ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: {name[:40]}...")
            
            desc = fetch_full_description(url)
            salary = item.get("salary")
            
            all_vacancies.append({
                "name": name,
                "category": classify_vacancy(name, desc),
                "company": item.get("employer", {}).get("name"),
                "salary_from": salary["from"] if salary else None,
                "salary_to": salary["to"] if salary else None,
                "currency": salary["currency"] if salary else None,
                "experience": item.get("experience", {}).get("name", "–ù–µ —É–∫–∞–∑–∞–Ω"),
                "url": url,
                "description": desc[:] + "..." # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è
            })
            time.sleep(0.1)
        
        progress_bar.progress((page + 1) / max_pages)
    
    status_container.success("‚úÖ –°–±–æ—Ä –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
    return pd.DataFrame(all_vacancies), None

# =========================================================
# 4. –ò–ù–¢–ï–†–§–ï–ô–° STREAMLIT
# =========================================================
st.set_page_config(page_title="HH Smart Parser 2026", layout="wide")
st.header("üîé –£–º–Ω—ã–π –ø–æ–∏—Å–∫ –ò–¢-–≤–∞–∫–∞–Ω—Å–∏–π")

if 'vacancies_df' not in st.session_state:
    st.session_state['vacancies_df'] = None
if 'selected_cat' not in st.session_state:
    st.session_state['selected_cat'] = "–í—Å–µ"

with st.form("parser_settings"):
    col1, col2, col3 = st.columns([2,2,1])
    city_in = col1.text_input("–ì–æ—Ä–æ–¥", value="–£—Ñ–∞")
    query_in = col2.text_input("–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ", value="Python")
    limit_in = col3.slider("–°—Ç—Ä–∞–Ω–∏—Ü", 1, 10, 2)
    submit = st.form_submit_button("–ù–∞—á–∞—Ç—å —Å–±–æ—Ä")

if submit:
    df_result, err = start_parsing(query_in, city_in, limit_in)
    if err: st.error(err)
    else:
        st.session_state['vacancies_df'] = df_result
        st.session_state['selected_cat'] = "–í—Å–µ"

if st.session_state['vacancies_df'] is not None:
    df = st.session_state['vacancies_df']
    
    # –°–µ–∫—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–ö–Ω–æ–ø–∫–∏-–ú–µ—Ç—Ä–∏–∫–∏)
    st.divider()
    cat_counts = df["category"].value_counts()
    st.write("### –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é:")
    
    c_all = st.columns(1)
    if c_all[0].button("üåê –ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏"):
        st.session_state['selected_cat'] = "–í—Å–µ"

    cols = st.columns(min(len(cat_counts), 5))
    for i, (cat, count) in enumerate(cat_counts.items()):
        if cols[i % 5].button(f"{cat}: {count}"):
            st.session_state['selected_cat'] = cat

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤—ã–≤–æ–¥–∞
    display_df = df if st.session_state['selected_cat'] == "–í—Å–µ" else df[df["category"] == st.session_state['selected_cat']]
    
    st.success(f"–í—ã–±—Ä–∞–Ω–æ: **{st.session_state['selected_cat']}** ({len(display_df)} —à—Ç.)")
    st.dataframe(
        display_df.drop(columns=['description']), 
        use_container_width=True,
        column_config={"url": st.column_config.LinkColumn("–°—Å—ã–ª–∫–∞")}
    )

    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False)
    st.download_button("üì• –°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–π Excel", output.getvalue(), f"vacancies_{city_in}.xlsx")
    st.info("üí° –¢–µ–ø–µ—Ä—å –¥–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ —Ä–∞–∑–¥–µ–ª–µ **–ê–Ω–∞–ª–∏—Ç–∏–∫–∞**.")