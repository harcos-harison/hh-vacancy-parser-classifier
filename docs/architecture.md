# Архитектура и стек технологий HH Parser

## 1. Цели проекта

- Собирать вакансии с HH.ru по городу или по всей России.
- Нормализовать и обогащать данные (описание, навыки, категория, опыт, зарплата).
- Показывать интерактивную аналитику рынка через удобный веб‑интерфейс.
- Давать возможность загружать внешние выгрузки (CSV/Excel/JSON) и прогонять их через те же аналитические пайплайны.

## 2. Общая архитектура

Архитектура состоит из нескольких слоёв:

1. **Сбор данных (парсер HH API)**  
   Модули `filter city/save_csv.py` и `filter city/save_csv_2.py`:
   - обращаются к API HH (`https://api.hh.ru/vacancies`, `https://api.hh.ru/areas`);
   - находят регион по названию города (`get_area_id_by_city`);
   - постранично собирают вакансии (`get_vacancies_by_region`, `get_all_vacancies`);
   - при необходимости дотягивают полные описания со страницы вакансии (`fetch_full_description`);
   - сохраняют результат в Excel (`save_vacancies_to_xlsx`).

2. **NLP и обогащение данных (Streamlit, Natasha)**  
   Главная страница `filter city/Chart/main_page/main.py`:
   - очищает и лемматизирует текст описания вакансии (`clean_and_lemmatize`) с помощью `natasha`;
   - выделяет технологии и навыки по словарю (`extract_skills` и `SKILL_MAP`);
   - классифицирует вакансию по направлению (`classify_vacancy` и `CATEGORIES`/`PRIORITY`);
   - формирует единый `pandas.DataFrame` с колонками:
     `name`, `category`, `company`, `salary_from`, `salary_to`, `currency`,
     `experience`, `skills`, `url`, `description`, `lemmatized_content`.
   - сохраняет датафрейм в `st.session_state['vacancies_df']` для других страниц.

3. **Интерактивная аналитика (Streamlit + Plotly)**  
   Навигация `filter city/Chart/main_nav.py` поднимает набор страниц:
   - `main_page/main.py` — запуск парсинга, таблица вакансий, экспорт в Excel, загрузка внешних файлов;
   - `salary_page/salary.py` — анализ зарплат (boxplot, медианные вилки, связь с опытом);
   - `experience_page/experience.py` — распределение требований по опыту и «порог входа» по направлениям;
   - `skills_page/skills.py` — популярность технологий и связь «навык ↔ средняя зарплата»;
   - `areas_page/area_page.py` — общее распределение по направлениям и базовые инсайты;
   - `experiments_page/experiments.py` — экспериментальные визуализации.

4. **Альтернативные интерфейсы (Tkinter, Flask)**  
   - `filter city/parser_ui.py` — десктоп‑клиент на Tkinter, использующий `save_csv_2.py`;
   - `filter city/app.py` — веб‑интерфейс на Flask (формы, выдача таблицы и экспорт).
   Они считаются дополнительными и могут отставать по функционалу от Streamlit‑версии.

5. **Эксперименты с ML‑моделями (Transformers)**  
   В `use model/` находятся скрипты, использующие `facebook/bart-large-mnli`:
   - проверка доступности GPU;
   - zero‑shot классификация вакансий по заранее заданным ролям;
   - сохранение результатов в отдельные CSV;
   - сейчас этот слой **не входит** в основной пайплайн MVP и рассматривается как задел на будущее.

## 3. Стек технологий и почему он выбран

### Язык и базовые библиотеки

- **Python 3.x** — де‑факто стандарт для data‑engineering/NLP/ML, богатая экосистема.
- **requests** — простой и понятный HTTP‑клиент для работы с API HH.
- **beautifulsoup4** — надёжный HTML‑парсер, используется для вытаскивания полного описания вакансии со страницы.
- **pandas** — удобная работа с табличными данными, агрегации и подготовка входа для визуализаций.
- **openpyxl / xlsxwriter** — сохранение и экспорт в Excel, что удобно для HR/аналитиков.

### Веб‑интерфейс и визуализация

- **Streamlit** (`filter city/Chart/`) — быстрый способ собрать аналитический дашборд:
  - декларативный код (минимум фронтенда);
  - удобная работа с состоянием (`st.session_state`);
  - встроенные компоненты для таблиц, загрузки файлов, кнопок и пр.;
  - идеален для прототипирования и MVP.
- **Plotly** — интерактивные графики:
  - бар‑чарты, boxplot, пай‑чанрты, линейные графики;
  - удобная интеграция со Streamlit через `st.plotly_chart`;
  - поддержка кастомных цветов и аннотаций, что важно для читабельности аналитики.
- **Flask** (`filter city/app.py`) — более низкоуровневый HTTP‑фреймворк:
  - использовался для ранней веб‑версии;
  - пригодится, если потребуется более кастомный backend или интеграция с внешними системами.
- **Tkinter** (`filter city/parser_ui.py`) — стандартная библиотека Python для GUI:
  - не требует дополнительных зависимостей;
  - позволяет запускать офлайн‑клиент без браузера;
  - текущая версия — дополнительный канал доступа к парсеру.

### NLP и классификация

- **Natasha** (`natasha`) — библиотека, заточенная под русский язык:
  - сегментация, морфологический анализ и лемматизация русскоязычных текстов;
  - критично для корректного поиска навыков и категорий в описаниях вакансий.
- **Heuristic‑based classification** (`SKILL_MAP`, `CATEGORIES`, `PRIORITY`):
  - быстрый и прозрачный способ разметить данные без обучения отдельной модели;
  - легко править и расширять словари без переобучения.
- **Transformers / torch** (`use model/`) — эксперименты:
  - `facebook/bart-large-mnli` для zero‑shot классификации;
  - рассматривается как возможный следующий этап, если точности эвристик будет недостаточно;
  - требуют GPU и тяжелее по ресурсам, поэтому пока не включены в стандартный путь.

## 4. Ключевые методы и точки расширения

- **Парсер HH API**
  - `get_area_id_by_city(city_name)` — поиск `area_id` по названию города.
  - `get_vacancies_by_region(text, area_id, per_page, page)` — постраничный запрос к HH API.
  - `parse_vacancies(data)` — приведение ответа HH к плоскому словарю/строке датафрейма.
  - `fetch_full_description(url)` — вытягивание HTML‑описания вакансии по ссылке.
  - `get_all_vacancies(text, city_name, per_page, max_pages)` — полный цикл сбора вакансий по городу.

- **Обработка и аналитика (Streamlit)**
  - `clean_and_lemmatize(text)` — нормализация и лемматизация описаний.
  - `extract_skills(lemmatized_text)` — поиск технологий/навыков в лемматизированном тексте.
  - `classify_vacancy(title, description_lemmatized)` — отнесение вакансии к направлению.
  - Страницы `salary.py`, `experience.py`, `skills.py`, `area_page.py` строят визуализации на основе единого датафрейма `vacancies_df`.

Основные точки расширения:

- дополнять словари `SKILL_MAP`, `CATEGORIES`, `PRIORITY` новыми технологиями и направлениями;
- менять стратегию кластеризации/классификации вакансий (вплоть до подключения ML‑модели);
- добавлять новые Streamlit‑страницы и визуализации поверх `st.session_state['vacancies_df']`;
- интегрировать сохранённые выгрузки и результаты анализа в внешние системы (БД, BI‑инструменты и т.п.).

## 5. Ограничения MVP

- Логика классификации основана на словарях и простых правилах, а не на обученной модели — возможны ошибки и «серые зоны».
- Парсер чувствителен к изменениям в API/HTML‑разметке HH.ru.
- При глубоком парсинге (много страниц и полный текст описания) возможна заметная задержка из‑за количества HTTP‑запросов и NLP‑обработки.

Эти ограничения планируется постепенно снимать по мере развития проекта (улучшение словарей, возможная интеграция ML‑модели, оптимизация пайплайна обработки).

